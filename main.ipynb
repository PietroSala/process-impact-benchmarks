{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPMN Process Generation\n",
    "\n",
    "We generated 10 BPMN processes for each combination of maximum nested XOR and maximum independent XOR, ranging from 1 to 10. This resulted in a total of 1000 unique processes (10 x 10 x 10). The generation process follows these general steps:\n",
    "\n",
    "1. Start with a seed string '_', which represents a single task.\n",
    "\n",
    "2. Iteratively replace underscores with one of three possible structures:\n",
    "   - XOR split: `(_ ^ _)`\n",
    "   - Parallel split: `(_ || _)`\n",
    "   - Sequential: `(_ , _)`\n",
    "\n",
    "3. The replacement process continues until the desired complexity (in terms of nested and independent XORs) is achieved.\n",
    "\n",
    "4. Finally, replace remaining underscores with task labels (T1, T2, etc.).\n",
    "\n",
    "The generation process uses weighted random choices to determine which structure to use for each replacement. This allows for controlled variability in the generated processes.\n",
    "\n",
    "Key aspects of the generation:\n",
    "\n",
    "- The `replace_random_underscore` function (in `random_diagram_generation.py`) performs the core replacement logic.\n",
    "- Probabilities for each type of replacement can be specified or randomly generated using `guess_three_numbers` (in `random_diagram_generation.py`).\n",
    "- The `max_nested_xor` and `max_independent_xor` functions (in `stats.py`) are used to evaluate the complexity of the generated processes.\n",
    "- The `generate_multiple_processes` function (in `random_batch_generation.py`) manages the overall generation process, ensuring the desired number of processes with specific complexities are created.\n",
    "\n",
    "This approach ensures a diverse set of BPMN processes with varying levels of complexity, suitable for thorough testing and analysis of process-related algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions and modules\n",
    "from random_diagram_generation import SEED_STRING, replace_random_underscore, replace_underscores, guess_three_numbers\n",
    "from sese_diagram import PARSER, print_sese_diagram, print_tree, dot_tree\n",
    "from stats import max_nested_xor, max_independent_xor\n",
    "\n",
    "# Generate a process\n",
    "current_string = SEED_STRING\n",
    "probabilities = guess_three_numbers()\n",
    "print(f\"Using probabilities: {probabilities}\")\n",
    "\n",
    "iterations = 10\n",
    "for _ in range(iterations):\n",
    "    current_string = replace_random_underscore(current_string, probabilities)\n",
    "\n",
    "process = replace_underscores(current_string)\n",
    "print(\"Generated Process:\")\n",
    "print(process)\n",
    "\n",
    "# Parse the process\n",
    "tree = PARSER.parse(process)\n",
    "\n",
    "# Print process statistics\n",
    "print(f\"\\nMax Nested XOR: {max_nested_xor(process)}\")\n",
    "print(f\"Max Independent XOR: {max_independent_xor(process)}\")\n",
    "\n",
    "# Visualize the process\n",
    "print(\"\\nProcess Diagram:\")\n",
    "print_sese_diagram(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the parse tree\n",
    "print(\"\\nProcess Tree:\")\n",
    "print_tree(dot_tree(tree)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Vector Generation\n",
    "\n",
    "After generating the BPMN processes, we create a set of impact vectors for each process. These impact vectors represent the multi-dimensional effects or outcomes of the tasks within each process. We use six different modes to generate these vectors, each providing a unique distribution of values. The impact vectors are generated using functions from `generate_impacts.py`.\n",
    "\n",
    "## Generation Modes\n",
    "\n",
    "1. **Random**: \n",
    "   - Generates vectors where each dimension is a random value between 0 and 1.\n",
    "   - No further modifications are made to these vectors.\n",
    "\n",
    "2. **Bagging Divide**: \n",
    "   - Starts with random vectors (values between 0 and 1).\n",
    "   - Randomly selects dimensions and divides their values by 10.\n",
    "   - This creates vectors with a mix of larger (original) and smaller (divided) impacts.\n",
    "\n",
    "3. **Bagging Remove**: \n",
    "   - Starts with random vectors (values between 0 and 1).\n",
    "   - Randomly selects dimensions and sets their values to 0.\n",
    "   - The result is a sparse vector where some dimensions have no impact.\n",
    "\n",
    "4. **Bagging Remove Divide**: \n",
    "   - Starts with random vectors (values between 0 and 1).\n",
    "   - Randomly selects dimensions and sets their values to 0.\n",
    "   - For the non-zero dimensions, randomly selects some and divides their values by 10.\n",
    "   - This creates sparse vectors with a mix of original and reduced non-zero values.\n",
    "\n",
    "5. **Bagging Remove Reverse**: \n",
    "   - Starts with random vectors (values between 0 and 1).\n",
    "   - Creates a new vector of zeros with the same dimension.\n",
    "   - Randomly selects dimensions from the original vector and copies their values to the new vector.\n",
    "   - This results in a sparse vector where the non-zero values retain their original magnitudes and positions.\n",
    "\n",
    "6. **Bagging Remove Reverse Divide**: \n",
    "   - Starts with random vectors (values between 0 and 1).\n",
    "   - Creates a new vector of zeros with the same dimension.\n",
    "   - Randomly selects dimensions from the original vector and copies their values to the new vector.\n",
    "   - Among the non-zero values in the new vector, randomly selects some and divides them by 10.\n",
    "   - This creates a sparse vector with a mix of original and reduced non-zero values in their original positions.\n",
    "\n",
    "## Process\n",
    "\n",
    "For each BPMN process:\n",
    "1. We determine the number of tasks in the process.\n",
    "2. We generate a set of impact vectors equal to the number of tasks, using one of the six modes described above.\n",
    "3. The dimension of each vector corresponds to the number of impact factors being considered (ranging from 1 to 10 in our experiments).\n",
    "4. These vectors are then associated with the tasks in the process in order of appearance.\n",
    "\n",
    "For example, if a process has 5 tasks, we generate 5 impact vectors using the chosen mode. Then we assign these vectors to the tasks in the order they appear in the process description.\n",
    "\n",
    "## Dimension Range\n",
    "\n",
    "In our experiments, we generate impact vectors with dimensions varying from 1 to 10. This range allows us to analyze how the number of impact factors affects the complexity and performance of our algorithms.\n",
    "\n",
    "## Association with Tasks\n",
    "\n",
    "The association of impact vectors with tasks is done by creating a dictionary where:\n",
    "- The keys are the task identifiers (e.g., \"T1\", \"T2\", \"T3\", etc.)\n",
    "- The values are the corresponding generated impact vectors\n",
    "\n",
    "This dictionary is then used in subsequent analyses to retrieve the impact vector for any given task in the process.\n",
    "\n",
    "By using these diverse generation modes, varying dimensions, and associating vectors with specific tasks, we create a comprehensive dataset that captures a wide range of possible impact distributions across different process structures. This approach enables thorough testing and analysis of our process impact evaluation algorithms across different scenarios and complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_impacts import generate_vectors\n",
    "from utils import find_max_task_number\n",
    "\n",
    "\n",
    "num_tasks = find_max_task_number(process)\n",
    "dimension = 3 \n",
    "\n",
    "vectors = generate_vectors(num_tasks, dimension, mode=\"random\")  # You can change the mode as needed\n",
    "\n",
    "impacts = {f\"T{i+1}\": vector.tolist() for i, vector in enumerate(vectors)}\n",
    "\n",
    "print(\"\\nProcess Diagram with Impact Vectors (truncated to 2 decimal places):\")\n",
    "print_sese_diagram(process, impacts=impacts, impacts_names=[f\"Impact {i+1}\" for i in range(dimension)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Generation and Analysis Process\n",
    "\n",
    "To analyze the characteristics of our impact vector generation methods, we will conduct a comprehensive study across different dimensions and generation modes. This process will help us understand how the different generation modes affect the distribution and relationships between the vectors.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "1. **Vector Generation**: \n",
    "   - For each dimension (1 to 10) and each of the six generation modes, we will generate 100 vectors.\n",
    "   - This results in 60 sets of 100 vectors each (10 dimensions × 6 modes).\n",
    "\n",
    "2. **Cosine Distance Calculation**:\n",
    "   - For each set of 100 vectors, we will compute the cosine distance between each pair of vectors.\n",
    "   - The cosine distance is calculated as:\n",
    "\n",
    "     $\\text{cosine\\_distance}(A, B) = 1 - \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$\n",
    "\n",
    "   where $A \\cdot B$ is the dot product of vectors A and B, and $\\|A\\|$ and $\\|B\\|$ are the magnitudes of vectors A and B respectively.\n",
    "\n",
    "   - **Important Note**: Since all our generated vectors contain only non-negative values, the cosine distance will always be between 0 and 1. This is because:\n",
    "     - The dot product of two non-negative vectors is always non-negative.\n",
    "     - The magnitudes of non-negative vectors are always positive.\n",
    "     - Therefore, the fraction $\\frac{A \\cdot B}{\\|A\\| \\|B\\|}$ is always between 0 and 1.\n",
    "     - Subtracting this fraction from 1 gives us a distance that is also between 0 and 1.\n",
    "\n",
    "3. **Statistical Analysis**:\n",
    "   - For each set of cosine distances (4950 distances per set, as there are $\\binom{100}{2} = 4950$ pairs in 100 vectors), we will compute:\n",
    "     - The mean cosine distance\n",
    "     - The standard deviation of the cosine distances\n",
    "\n",
    "4. **Visualization**:\n",
    "   - We will create a scatter plot to visualize the results:\n",
    "     - X-axis: Dimensions (1 to 10)\n",
    "     - Y-axis: Mean cosine distance\n",
    "     - Error bars: Representing the standard deviation, clamped to the [0, 1] range\n",
    "     - Different colors and symbols for each generation mode\n",
    "\n",
    "## Generation Modes\n",
    "\n",
    "We will use the following six modes for vector generation, in this order:\n",
    "\n",
    "1. Random\n",
    "2. Bagging Divide\n",
    "3. Bagging Remove\n",
    "4. Bagging Remove Divide\n",
    "5. Bagging Remove Reverse\n",
    "6. Bagging Remove Reverse Divide\n",
    "\n",
    "## Expected Insights\n",
    "\n",
    "This analysis will allow us to:\n",
    "- Understand how the different generation modes affect the similarity/dissimilarity between vectors.\n",
    "- Observe how the dimensionality of the vectors impacts the distances between them.\n",
    "- Identify which generation modes produce more diverse or more similar sets of vectors.\n",
    "- Determine how the variability in distances changes across dimensions and generation modes.\n",
    "\n",
    "These insights will be crucial for selecting appropriate vector generation methods for different scenarios in our BPMN process impact analysis. The bounded nature of our cosine distances, between 0 and 1 since we allow positive vectors, provides a consistent scale for comparing the different modes and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run impact_stats.py\n",
    "\n",
    "# Run the analysis and plot the results\n",
    "results, modes = generate_and_analyze_vectors(num_vectors=100, max_dim=10)\n",
    "plot_mean_std_intervals_by_dimension(results, modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👁️ Observation on Distances among Distinct Generation Strategies 👁️\n",
    "\n",
    "Our objective is to generate sets of arrays whose elements are generally very distant from each other, and observe whether such dissimilarity influences the computational time, i.e., the size of the maximum Pareto frontier encountered during the execution of the algorithm. We hypothesize that the more \"spread apart\" the initial impact vectors associated with the tasks are (with respect to cosine distance), the longer the algorithm will take to compute the final Pareto frontier.\n",
    "\n",
    "Let us evaluate the above results in terms of the proposed 6 generation strategies:\n",
    "\n",
    "- The ```random``` modality generates sets of very similar arrays.\n",
    "- ```bagging_divide``` creates generally less similar sets compared to ```bagging_remove``` and ```bagging_remove_divide```. Quite surprisingly, this trend is maintained even as we increase the dimension, indicating that when the impact arrays are not sparse, it is better to divide randomly than to set a few components to $0$. Surprisingly, dividing and setting a few components to $0$ worsens the spread.\n",
    "- ```bagging_remove``` and ```bagging_remove_divide```, for the aforementioned reasons, have more spread than random but they still produce still fairly similar impact vectors.\n",
    "- ```bagging_remove_reverse``` becomes slightly better than ```bagging_divide``` in terms of spreading as the dimension increases to 4 or more. Note that such arrays are sparse but maintain the same scale for the components since components are not randomly selected for division by $10$. It represents a slightly more difficult case than ```bagging_divide``` when dimension grows, despite the fact that components are not scaled.\n",
    "- ```bagging_remove_reverse_divide``` generates significantly more spread-out sets of vectors as the dimension grows beyond 7. It represents the most appropriate strategy among the 6 proposed for generating worst-case scenarios.\n",
    "\n",
    "As a final qualitative observation, we would like to point out that ```bagging_remove_reverse_divide``` represents many realistic scenarios where a single task impacts a small percentage of the total number of components, and the way different tasks affect the same component may be on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py\n",
    "\n",
    "nested = 4\n",
    "independent = 10\n",
    "process_number = 1\n",
    "p = get_process_from_file(nested,independent,process_number)\n",
    "print_sese_diagram(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run algorithms.py\n",
    "\n",
    "dimension = 7\n",
    "r = {}\n",
    "for mode in [\"random\", \"bagging_divide\", \"bagging_remove\",\"bagging_remove_divide\",\"bagging_remove_reverse\", \"bagging_remove_reverse_divide\"]:\n",
    "    n = find_max_task_number(p)\n",
    "    v = generate_vectors(n,dimension, mode=mode)\n",
    "    imp = dict(zip([f\"T{i}\" for i in range(1,n+1)], v))\n",
    "    t = PARSER.parse(p)\n",
    "    build_pareto_frontier(t, imp)\n",
    "    r[mode] = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_tree(dot_tree(r[\"random\"], pareto=True)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(dot_tree(r[\"bagging_divide\"], pareto=True)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(dot_tree(r[\"bagging_remove\"], pareto=True)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(dot_tree(r[\"bagging_remove_divide\"], pareto=True)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(dot_tree(r[\"bagging_remove_reverse\"], pareto=True)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(dot_tree(r[\"bagging_remove_reverse_divide\"], pareto=True)[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👁️ Observation on Generated Process 👁️\n",
    "\n",
    "In the above example, you can pick a process from the dataset and a dimension to visualize the Pareto frontier for distinct modes. Usually, the largest Pareto frontier is associated with the root of the region tree, and generating it represents the maximum requirement in terms of computational time.\n",
    "\n",
    "In the given example, we observe that the Pareto frontier for the ```random``` generation strategy is significantly smaller compared to the other generation strategies. The ```bagging_remove``` and ```bagging_remove_divide``` strategies generate similar frontiers, slightly smaller than the ```bagging_divide``` one, which is comparable to the ```bagging_remove_reverse```. The ```bagging_remove_reverse_divide``` strategy generates the largest Pareto frontier by far.\n",
    "\n",
    "This small experiment provides evidence for our assumption about how different strategies affect the size of the Pareto frontier. More generally, it provides evidence how the spreading of the impact vectors with respect to the cosine distance influences the frontier's size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "We conducted a comprehensive experiment to analyze the impact of various factors on process complexity and performance. The experiment was implemented as described in `script.py` and covers a wide range of parameters:\n",
    "\n",
    "## Parameters\n",
    "\n",
    "1. **Dimensions (dim)**: 1 to 10\n",
    "   - This represents the number of dimensions in the impact vectors associated with each task.\n",
    "\n",
    "2. **Maximum Nested XOR (nested)**: 1 to 10\n",
    "   - This parameter indicates the maximum depth of nested XOR gateways in the generated BPMN processes.\n",
    "\n",
    "3. **Maximum Independent XOR (independent)**: 1 to 10\n",
    "   - This represents the maximum number of independent XOR gateways at the same level in the generated BPMN processes.\n",
    "\n",
    "4. **Process Number**: 1 to 10\n",
    "   - For each combination of nested and independent XOR, we generated 10 different processes.\n",
    "\n",
    "5. **Vector Generation Modes**:\n",
    "   - random\n",
    "   - bagging_divide\n",
    "   - bagging_remove\n",
    "   - bagging_remove_divide\n",
    "   - bagging_remove_reverse\n",
    "   - bagging_remove_reverse_divide\n",
    "\n",
    "## Experiment Structure\n",
    "\n",
    "For each combination of the above parameters, we performed the following steps:\n",
    "\n",
    "1. Generated a BPMN process based on the nested and independent XOR parameters.\n",
    "2. Created impact vectors for each task in the process using the specified dimension and generation mode.\n",
    "3. Computed the maximum values and Pareto frontier for the process.\n",
    "4. Measured the computation time for both the max values and Pareto frontier calculations.\n",
    "\n",
    "In total, this experiment covers:\n",
    "- 10 (dimensions) × 10 (nested XOR) × 10 (independent XOR) × 10 (process numbers) × 6 (generation modes) = 60,000 unique configurations\n",
    "\n",
    "## Pareto Frontier and Domination Definition\n",
    "\n",
    "An important concept in our analysis is the Pareto frontier, specifically the bottom Pareto frontier. Given a set of vectors, we define domination and the bottom Pareto frontier as follows:\n",
    "\n",
    "### Domination\n",
    "\n",
    "A vector $u$ is said to be dominated by another vector $v$ if:\n",
    "\n",
    "1. For all dimensions, the value in $u$ is less than or equal to the corresponding value in $v$, AND\n",
    "2. For at least one dimension, the value in $u$ is strictly less than the corresponding value in $v$.\n",
    "\n",
    "Mathematically, for n-dimensional vectors $u$ and $v$, $u$ is dominated by $v$ if and only if:\n",
    "\n",
    "$\\forall i \\in \\{1, ..., n\\} : u[i] \\leq v[i] \\land \\exists j \\in \\{1, ..., n\\} : u[j] < v[j]$\n",
    "\n",
    "### Bottom Pareto Frontier\n",
    "\n",
    "The bottom Pareto frontier (also known as the minimal Pareto set or minimal Pareto front) is the set of vectors that do not dominate any other vector in the set. \n",
    "\n",
    "Mathematically, for a set of n-dimensional vectors $V$, the bottom Pareto frontier $P$ is defined as:\n",
    "\n",
    "$P = \\{v \\in V \\mid \\neg\\exists u \\in V : u \\text{ is dominated by } v\\}$\n",
    "\n",
    "In other words, the bottom Pareto frontier consists of all vectors in $V$ that do not dominate any other vector in $V$.\n",
    "\n",
    "In the context of our BPMN process analysis:\n",
    "- Each vector represents the cumulative impact of a particular execution path through the process.\n",
    "- The dimensions of the vectors correspond to different impact factors (e.g., time, cost, quality).\n",
    "- We consider lower values as more desirable for our impact factors.\n",
    "- The bottom Pareto frontier represents the set of execution paths that do not dominate any other paths, each of which is optimal in the sense that there's no other path that's better in all impact factors.\n",
    "\n",
    "## Significance in Our Experiment\n",
    "\n",
    "In our experiment:\n",
    "- We generate impact vectors for each task, where lower values represent better outcomes.\n",
    "- When we compute the Pareto frontier for a process, we're identifying the set of execution paths that are Pareto-optimal (i.e., do not dominate any other path).\n",
    "- By using this domination definition, we identify the bottom Pareto frontier, which represents the best execution paths in terms of minimizing impact factors.\n",
    "- The maximum length of the Pareto frontier gives us an indication of the complexity of trade-offs in the process – a longer frontier suggests more diverse, incomparable optimal solutions.\n",
    "\n",
    "This approach allows us to analyze how different process structures and impact vector characteristics affect the set of optimal execution paths and the complexity of decision-making in process execution.\n",
    "\n",
    "By focusing on vectors that do not dominate any others, we identify execution paths that offer unique trade-offs in minimizing impact factors, providing valuable insights into process optimization.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "For each run, we collected the following data and stored it in columns in our results.csv file:\n",
    "\n",
    "- Nested XOR level (column: ```nested```)\n",
    "- Independent XOR level (column: ```independent```)\n",
    "- Process number (column: ```process_number```)\n",
    "- Number of dimensions (column: ```dimensions```)\n",
    "- Vector generation mode (column: ```mode```)\n",
    "- Maximum length of the Pareto frontier (column: ```max_pareto```)\n",
    "- Maximum theoretical length of the Pareto frontier (column: ```max_theoretical_pareto```)\n",
    "- Computation time for max values (column: ```max_time```)\n",
    "- Computation time for Pareto frontier (column: ```pareto_time```)\n",
    "\n",
    "This data enables us to perform detailed analyses on the relationships between process structure, impact vector characteristics, and algorithmic performance.\n",
    "\n",
    "## 💻 Computational Environment 🖥️\n",
    "\n",
    "The computational times reported in this experiment were measured on a system with the following specifications:\n",
    "\n",
    "- **Operating System**: Ubuntu 22.04.3 LTS\n",
    "- **Processor**: Intel(R) Core(TM) i9-10980HK CPU @ 2.40GHz\n",
    "- **Memory**: 32GB RAM\n",
    "\n",
    "## ⚠️ Very Important Notes ⚠️\n",
    "\n",
    "The following points are crucial for understanding the relationship between our experimental data and the problems addressed in the associated paper:\n",
    "\n",
    "1. **Column \"max_time\"**: This represents the computation time for determining the maximal values for each component. It corresponds to checking if all paths respect a given bound, which is **Problem 2** as described in the paper.\n",
    "\n",
    "2. **Problems 1 and 3 for single dimension**: Problem 1 with $k = 1$ and Problem 3 from the paper are equivalent to computing the Pareto frontier when the number of dimensions (dim) is 1.\n",
    "\n",
    "3. **Pareto frontier as an upper bound**: For Problem 1 with $k > 1$ and Problem 3, the computation of the Pareto frontier serves as an upper bound for the computational time. This is because:\n",
    "   - The Pareto frontier computation does not utilize the input_bound, which could potentially lead to a speed-up in the algorithm for these specific problems.\n",
    "   - We chose to compute the full Pareto frontier instead of directly solving Problem 1 with $k > 1$ and Problem 3 to avoid introducing and managing an additional parameter (input_bound) in our experimental setup.\n",
    "   - The computational time for the Pareto frontier (column: \"pareto_time\") thus provides a conservative estimate for the time required to solve these problems.\n",
    "\n",
    "These notes are essential for correctly interpreting the results and understanding the scope of our analysis in relation to the theoretical problems discussed in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❌ Missing Experiments ❌ \n",
    "\n",
    "The following function prints the ```nested, independent, process_number, dimensions,mode```  assignments for which\n",
    "we do not have experimental data due to excessive computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run computation_stats.py\n",
    "\n",
    "missing_values(df, 9,10,8,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run computation_stats.py\n",
    "\n",
    "independent_name = \"independent\"\n",
    "nested_name = \"nested\"\n",
    "\n",
    "colorbar = dict(\n",
    "                colorbar_x_left=0.45,\n",
    "                colorbar_y_left=-0.3,\n",
    "                colorbar_x_right=0.99,\n",
    "                colorbar_y_right=-0.4\n",
    "                )\n",
    "\n",
    "create_log_heatmaps(df, independent_name, nested_name, \"pareto_time\", \"Pareto Time\", width=1200, **colorbar)\n",
    "\n",
    "create_log_heatmaps(df, independent_name, nested_name, \"max_pareto\", \"Max Pareto Region\", width=1200, **colorbar)\n",
    "\n",
    "create_log_heatmaps(df, independent_name, nested_name, \"max_theoretical_pareto\", \"Max Theoretical Pareto Region Size\", width=1200, **colorbar)\n",
    "\n",
    "create_log_heatmaps(df, independent_name, nested_name, \"max_time\", \"Max Component Time\", width=1200, **colorbar, log=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👁️ Observation on Heatmaps for Experiment Metrics 👁️\n",
    "\n",
    "In the above analysis, we provide heatmaps for the experiment metrics ```pareto_time```, ```max_pareto```, ```max_theoretical_pareto```, and ```max_time```, respectively. The heat is represented on a logarithmic scale due to the exponential nature of the problem, except for the heatmaps of ```max_time```, since this problem is polynomial.\n",
    "\n",
    "Processes are grouped in each cell of the heatmap according to their ```independent``` and ```nested``` parameters, which represent the maximum number of independent XORs and the maximum number of nested XORs, respectively. For each metric, we present two heatmaps: one showing the average of the metric, and its twin showing the standard deviation.\n",
    "\n",
    "From this analysis, we can make two main observations:\n",
    "\n",
    "1. Both the average and standard deviation are affected more significantly when we increase ```independent``` and ```nested``` together, rather than increasing just one of the two parameters.\n",
    "\n",
    "2. The growing standard deviation is likely due to the fact that generation strategies and dimensions are not accounted for in this initial analysis. That is, all processes with the same combination of ```independent``` and ```nested``` are placed in the same bucket, regardless of their generation strategy or dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automata_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
